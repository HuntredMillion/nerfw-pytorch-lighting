{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os\n",
    "# point Python at the cloned repo\n",
    "sys.path.append(os.path.abspath(\"nerfw-pytorch\"))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# options parser\n",
    "from option.nerf_option import NeRFOption\n",
    "# dataset (for Cambridge; swap for 7‑Scenes as needed)\n",
    "from dataset.cambridge import CambridgeDataset\n",
    "# the NeRFW system\n",
    "from model.nerfw_system import NeRFWSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modified second cell\n",
    "sys.argv = [\"\"] + [\n",
    "    # ——— Training / experiment setup ———\n",
    "    \"--is_train\",      \"True\",                       # train mode (False for inference)\n",
    "    \"--root_dir\",      \"./runs/nerf\",                # where to write logs & checkpoints\n",
    "    \"--exp_name\",      \"my_experiment\",              # subfolder under root_dir for this run\n",
    "\n",
    "    # ——— Data loading ———\n",
    "    \"--data_root_dir\", \"./data/Cambridge\",           # top‑level data folder\n",
    "    \"--scene\",         \"KingsCollege\",               # which scene subfolder to use\n",
    "    \"--img_downscale\", \"3\",                          # downsample factor for images\n",
    "\n",
    "    # ——— Batch & sampling ———\n",
    "    \"--batch_size\",    \"1024\",                       # rays per optimization step\n",
    "    \"--chunk\",         \"4096\",                       # max rays per network forward\n",
    "    \"--N_c\",           \"64\",                         # # coarse samples per ray\n",
    "    \"--N_f\",           \"128\",                        # # fine samples per ray\n",
    "    \"--perturb\",       \"1.0\",                        # stratified sampling noise\n",
    "\n",
    "    # ——— Model & regularization ———\n",
    "    \"--encode_a\",      \"True\",                       # enable appearance embedding\n",
    "    \"--encode_t\",      \"True\",                       # enable transient embedding\n",
    "    \"--a_dim\",         \"48\",                         # appearance embedding size\n",
    "    \"--t_dim\",         \"16\",                         # transient embedding size\n",
    "    \"--beta_min\",      \"0.1\",                        # min transient variance\n",
    "    \"--lambda_u\",      \"0.01\",                       # weight on transient regularizer\n",
    "\n",
    "    # ——— Optimization ———\n",
    "    \"--lr\",            \"5e-4\",                       # initial learning rate\n",
    "    \"--epochs\",        \"20\",                         # number of training epochs\n",
    "    \"--num_gpus\",      \"1\",                          # number of GPUs (DDP if >1)\n",
    "\n",
    "    # ——— Caching ———\n",
    "    \"--use_cache\",     False,                      # build ray cache?\n",
    "    \"--if_save_cache\", \"True\",                       # save ray cache for next runs\n",
    "\n",
    "    # ——— Checkpointing & logging ———\n",
    "    \"--save_latest_freq\", \"1000\",                    # iters between \"latest\" saves\n",
    "    # Removed --save_epoch_freq as it's not in the original options\n",
    "\n",
    "    # ——— (Inference only) ———\n",
    "    \"--ckpt_path\",     \"checkpoints/nerfw_epoch20.pth\",  # Changed --ckpt to --ckpt_path\n",
    "    \"--last_epoch\",    \"0\"                          # epoch index corresponding to ckpt\n",
    "]\n",
    "\n",
    "opt = NeRFOption().into_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(opt.use_cache)\n",
    "print(opt.if_save_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using M2 GPU via Metal!\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using M2 GPU via Metal!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS device not found, using CPU!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load reconstruction data of scene \"KingsCollege\", split: train\n",
      "all rays:  torch.Size([281088000, 12])\n",
      "cache load done...\n",
      "Epoch: 0, Batch: 0, Loss: 2.5382\n",
      "Epoch: 0, Batch: 1000, Loss: 1.6158\n",
      "Epoch: 0, Batch: 2000, Loss: 1.4260\n",
      "Epoch: 0, Batch: 3000, Loss: 1.2899\n",
      "Epoch: 0, Batch: 4000, Loss: 1.2964\n",
      "Epoch: 0, Batch: 5000, Loss: 1.1919\n",
      "Epoch: 0, Batch: 6000, Loss: 1.1758\n",
      "Epoch: 0, Batch: 7000, Loss: 1.1442\n",
      "Epoch: 0, Batch: 8000, Loss: 1.1713\n",
      "Epoch: 0, Batch: 9000, Loss: 1.1829\n",
      "Epoch: 0, Batch: 10000, Loss: 1.1302\n",
      "Epoch: 0, Batch: 11000, Loss: 1.1570\n",
      "Epoch: 0, Batch: 12000, Loss: 1.1639\n",
      "Epoch: 0, Batch: 13000, Loss: 1.1263\n",
      "Epoch: 0, Batch: 14000, Loss: 1.1477\n",
      "Epoch: 0, Batch: 15000, Loss: 1.1532\n",
      "Epoch: 0, Batch: 16000, Loss: 1.1172\n",
      "Epoch: 0, Batch: 17000, Loss: 1.0882\n",
      "Epoch: 0, Batch: 18000, Loss: 1.1093\n",
      "Epoch: 0, Batch: 19000, Loss: 1.0841\n",
      "Epoch: 0, Batch: 20000, Loss: 1.0808\n",
      "Epoch: 0, Batch: 21000, Loss: 1.1059\n",
      "Epoch: 0, Batch: 22000, Loss: 1.0822\n",
      "Epoch: 0, Batch: 23000, Loss: 1.0474\n",
      "Epoch: 0, Batch: 24000, Loss: 1.0635\n",
      "Epoch: 0, Batch: 25000, Loss: 1.0997\n",
      "Epoch: 0, Batch: 26000, Loss: 1.0822\n",
      "Epoch: 0, Batch: 27000, Loss: 1.0816\n",
      "Epoch: 0, Batch: 28000, Loss: 1.0497\n",
      "Epoch: 0, Batch: 29000, Loss: 1.0635\n",
      "Epoch: 0, Batch: 30000, Loss: 1.1022\n",
      "Epoch: 0, Batch: 31000, Loss: 1.0667\n",
      "Epoch: 0, Batch: 32000, Loss: 1.0549\n",
      "Epoch: 0, Batch: 33000, Loss: 1.0824\n",
      "Epoch: 0, Batch: 34000, Loss: 1.0559\n",
      "Epoch: 0, Batch: 35000, Loss: 1.0619\n",
      "Epoch: 0, Batch: 36000, Loss: 1.0516\n",
      "Epoch: 0, Batch: 37000, Loss: 1.0515\n",
      "Epoch: 0, Batch: 38000, Loss: 1.0334\n",
      "Epoch: 0, Batch: 39000, Loss: 1.0519\n",
      "Epoch: 0, Batch: 40000, Loss: 1.0444\n",
      "Epoch: 0, Batch: 41000, Loss: 1.0701\n",
      "Epoch: 0, Batch: 42000, Loss: 1.0630\n",
      "Epoch: 0, Batch: 43000, Loss: 1.0544\n",
      "Epoch: 0, Batch: 44000, Loss: 1.0415\n",
      "Epoch: 0, Batch: 45000, Loss: 1.0458\n",
      "Epoch: 0, Batch: 46000, Loss: 1.0396\n",
      "Epoch: 0, Batch: 47000, Loss: 1.0654\n",
      "Epoch: 0, Batch: 48000, Loss: 1.0287\n",
      "Epoch: 0, Batch: 49000, Loss: 1.0589\n",
      "Epoch: 0, Batch: 50000, Loss: 1.0352\n",
      "Epoch: 0, Batch: 51000, Loss: 1.0189\n",
      "Epoch: 0, Batch: 52000, Loss: 1.0555\n",
      "Epoch: 0, Batch: 53000, Loss: 1.0432\n",
      "Epoch: 0, Batch: 54000, Loss: 1.0464\n",
      "Epoch: 0, Batch: 55000, Loss: 1.0224\n",
      "Epoch: 0, Batch: 56000, Loss: 1.0466\n",
      "Epoch: 0, Batch: 57000, Loss: 1.0208\n",
      "Epoch: 0, Batch: 58000, Loss: 1.0021\n",
      "Epoch: 0, Batch: 59000, Loss: 1.0146\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfine_regular\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfine_regular\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 83\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m opt\u001b[38;5;241m.\u001b[39msave_latest_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dataset.cambridge import CambridgeDataset  # Note: it's cambridge.py not cambridge_dataset.py\n",
    "from torch.utils.data import DataLoader\n",
    "from model.nerfw_system import NeRFWSystem\n",
    "\n",
    "# 1. build dataloader\n",
    "train_dataset = CambridgeDataset(\n",
    "    root_dir=opt.data_root_dir,\n",
    "    scene=opt.scene,\n",
    "    split='train',\n",
    "    img_downscale=opt.img_downscale,\n",
    "    use_cache=True,\n",
    "    if_save_cache=opt.if_save_cache\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# 2. init model & optimizer\n",
    "model = NeRFWSystem(\n",
    "    N_views=len(train_dataset.train_set),  # Number of training views\n",
    "    N_c=opt.N_c,  # Number of coarse samples\n",
    "    N_f=opt.N_f,  # Number of fine samples\n",
    "    use_disp=opt.use_disp,\n",
    "    perturb=opt.perturb,\n",
    "    layers=opt.layers,\n",
    "    W=opt.W,\n",
    "    N_xyz_freq=opt.N_xyz_freq,\n",
    "    N_dir_freq=opt.N_dir_freq,\n",
    "    encode_a=opt.encode_a,\n",
    "    encode_t=opt.encode_t,\n",
    "    a_dim=opt.a_dim,\n",
    "    t_dim=opt.t_dim,\n",
    "    res_layer=[4],  # Default value from the code\n",
    "    device=device,\n",
    "    beta_min=opt.beta_min,\n",
    "    lambda_u=opt.lambda_u,\n",
    "    white_back=False  # Default value\n",
    ")\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.cuda()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
    "\n",
    "# 3. training loop\n",
    "for epoch in range(opt.epochs):\n",
    "    model.train()\n",
    "    for batch_idx, rays in enumerate(train_loader):\n",
    "        # if torch.cuda.is_available():\n",
    "        #     rays = rays.cuda()\n",
    "        rays = rays.to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # # The rays tensor contains all information:\n",
    "        # # [rays_o, rays_d, near, far, view_id, rgb]\n",
    "        # rays_o = rays[..., :3]\n",
    "        # rays_d = rays[..., 3:6]\n",
    "        # near = rays[..., 6:7]\n",
    "        # far = rays[..., 7:8]\n",
    "        # view_id = rays[..., 8:9]\n",
    "        # target = rays[..., 9:12]  # RGB values\n",
    "\n",
    "        # outputs = model(rays_o, rays_d, near, far, view_id)\n",
    "        # loss = model.get_loss(outputs, target)\n",
    "        # Pass the rays directly to the model\n",
    "\n",
    "        rays_input = rays[..., :9]  # First 9 dimensions [position, direction, near, far, id]\n",
    "        target = rays[..., 9:12]    # Last 3 dimensions [RGB]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        res_c, res_f, losses = model(rays_input, target, cal_loss=True)\n",
    "        loss = losses['coarse'] + losses['fine']\n",
    "        if losses['fine_regular'] is not None:\n",
    "            loss += losses['fine_regular']\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % opt.save_latest_freq == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Save checkpoint at the end of each epoch\n",
    "    if not os.path.exists(os.path.join(opt.root_dir, opt.exp_name)):\n",
    "        os.makedirs(os.path.join(opt.root_dir, opt.exp_name))\n",
    "\n",
    "    checkpoint_path = os.path.join(opt.root_dir, opt.exp_name, f'epoch_{epoch}.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
